






コンテンツへスキップ
ナビゲーションに移動










					e-dash



資料請求デモ予約専門家に相談ログイン
ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper
運営会社Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン





無料
お問い合わせ













e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ







実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ







e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ







				ログイン






seminarセミナー
トップ/セミナー

















SHARE
























				あらゆる企業が、自社に適した道筋で
				事業の成長と脱炭素化を両方実現できるように。



				ご希望のお取り組み内容に応じたご提案が可能です。詳しくはお問い合わせください。




資料を請求する


デモを予約する


専門家に相談する











ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper






PRODUCT-SITE
外部サイトへ遷移します。





























運営会社


プライバシーポリシー


accel.


accel DB


Carbon Offset





認証番号：JP23/00000419

					〒107-0052
					東京都港区赤坂四丁目8番18号
					赤坂JEBL 6階



				© 2023 e-dash Co., Ltd.



MENUニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper
運営会社Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン








コンテンツへスキップ
ナビゲーションに移動





					e-dash



資料請求デモ予約専門家に相談ログイン
ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper
運営会社Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン





無料
お問い合わせ













e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ







実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ







e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ







				ログイン








					e-dash



資料請求デモ予約専門家に相談ログイン
ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper
運営会社Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン





無料
お問い合わせ













e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ







実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ







e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ







				ログイン






					e-dash



資料請求デモ予約専門家に相談ログイン
資料請求
デモ予約
専門家に相談
ログイン
ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper
運営会社Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン



ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper
運営会社Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン



ニュースNews
ニュース
News
プロダクトProducts
プロダクト
Products
セミナーSeminar
セミナー
Seminar
ストーリーStory
ストーリー
Story
お役立ち資料White Paper
お役立ち資料
White Paper
運営会社Company
運営会社
Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン


右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン

資料請求
デモ予約
専門家に相談
ログイン



無料
お問い合わせ













e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ







実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ







e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ







				ログイン




無料
お問い合わせ













e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ







実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ







e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ







無料
お問い合わせ





無料
お問い合わせ












e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ







実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ







e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ












e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ







実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ







e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ










e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ







実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ







e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ








e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ






e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ





e-dashのサービス概要や導入後のサポートの流れなどをご覧いただけます
資料請求へ





実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ






実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ





実際の利用画面をご覧いただきながら「e-dash」を説明します
デモ予約へ





e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ






e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ





e-dashの専門家チームへお困りの内容をお気軽にご相談ください
専門家相談へ

				ログイン



seminarセミナー
トップ/セミナー



seminarセミナー
トップ/セミナー

seminarセミナー
seminar
セミナー

トップ/セミナー
/
セミナー















SHARE


































SHARE
































SHARE





























SHARE

























SHARE























SHARE






















SHARE


















				あらゆる企業が、自社に適した道筋で
				事業の成長と脱炭素化を両方実現できるように。



				ご希望のお取り組み内容に応じたご提案が可能です。詳しくはお問い合わせください。




資料を請求する


デモを予約する


専門家に相談する







				あらゆる企業が、自社に適した道筋で
				事業の成長と脱炭素化を両方実現できるように。



				ご希望のお取り組み内容に応じたご提案が可能です。詳しくはお問い合わせください。




資料を請求する


デモを予約する


専門家に相談する





				あらゆる企業が、自社に適した道筋で
				事業の成長と脱炭素化を両方実現できるように。



				ご希望のお取り組み内容に応じたご提案が可能です。詳しくはお問い合わせください。



				ご希望のお取り組み内容に応じたご提案が可能です。詳しくはお問い合わせください。




資料を請求する


デモを予約する


専門家に相談する









ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper









ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper







ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper

ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper

ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper

ニュースNews
ニュース
News
プロダクトProducts
プロダクト
Products
セミナーSeminar
セミナー
Seminar
ストーリーStory
ストーリー
Story
お役立ち資料White Paper
お役立ち資料
White Paper



PRODUCT-SITE
外部サイトへ遷移します。



























PRODUCT-SITE
外部サイトへ遷移します。

























PRODUCT-SITE
外部サイトへ遷移します。

PRODUCT-SITE
外部サイトへ遷移します。


















































運営会社


プライバシーポリシー


accel.


accel DB


Carbon Offset





認証番号：JP23/00000419

					〒107-0052
					東京都港区赤坂四丁目8番18号
					赤坂JEBL 6階



				© 2023 e-dash Co., Ltd.







運営会社


プライバシーポリシー


accel.


accel DB


Carbon Offset





認証番号：JP23/00000419

					〒107-0052
					東京都港区赤坂四丁目8番18号
					赤坂JEBL 6階



				© 2023 e-dash Co., Ltd.





運営会社


プライバシーポリシー


accel.


accel DB


Carbon Offset





認証番号：JP23/00000419

					〒107-0052
					東京都港区赤坂四丁目8番18号
					赤坂JEBL 6階



				© 2023 e-dash Co., Ltd.




認証番号：JP23/00000419

					〒107-0052
					東京都港区赤坂四丁目8番18号
					赤坂JEBL 6階




					〒107-0052
					東京都港区赤坂四丁目8番18号
					赤坂JEBL 6階


				© 2023 e-dash Co., Ltd.

MENU
ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper
運営会社Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン



ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper
運営会社Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン



ニュースNews
プロダクトProducts
セミナーSeminar
ストーリーStory
お役立ち資料White Paper
運営会社Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン



ニュースNews
ニュース
News
プロダクトProducts
プロダクト
Products
セミナーSeminar
セミナー
Seminar
ストーリーStory
ストーリー
Story
お役立ち資料White Paper
お役立ち資料
White Paper
運営会社Company
運営会社
Company
右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン


右側に配置するメニュー

資料請求
デモ予約
専門家に相談
ログイン

資料請求
デモ予約
専門家に相談
ログイン

headerZenn
divZenn
divZenn
aZenn
path
path
articleCykinso's Tech BlogCykinso's Tech BlogPublicationへの投稿🦜🦜🔗 LangChain v0.2.5 + OpenAI での RAG を用いた ChatBot 実装例yamasaKit2024/06/20に公開2024/06/28chatbotOpenAILangChainLLMRAGtech
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。
yamasaKitcheminformatics, machine learning, board gameCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 バッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈るDiscussionyamasaKitcheminformatics, machine learning, board gameバッジを贈るバッジを贈るとは目次背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
asideCykinso's Tech Blog
divCykinso's Tech Blog
divCykinso's Tech Blog
divCykinso's Tech Blog
aCykinso's Tech Blog
span
img
spanCykinso's Tech Blog
div
div
button
svg
path
path
g
divCykinso's Tech BlogPublicationへの投稿
divCykinso's Tech BlogPublicationへの投稿
divCykinso's Tech BlogPublicationへの投稿
aCykinso's Tech Blog
span
img
spanCykinso's Tech Blog
spanPublicationへの投稿
aPublicationへの投稿
header🦜🦜🔗 LangChain v0.2.5 + OpenAI での RAG を用いた ChatBot 実装例yamasaKit2024/06/20に公開2024/06/28
div🦜🦜🔗 LangChain v0.2.5 + OpenAI での RAG を用いた ChatBot 実装例yamasaKit2024/06/20に公開2024/06/28
div🦜🦜🔗 LangChain v0.2.5 + OpenAI での RAG を用いた ChatBot 実装例yamasaKit2024/06/20に公開2024/06/28
div🦜
span
span
span🦜
h1🦜🔗 LangChain v0.2.5 + OpenAI での RAG を用いた ChatBot 実装例
divyamasaKit2024/06/20に公開2024/06/28
divyamasaKit2024/06/20に公開2024/06/28
divyamasaKit
a
img
ayamasaKit
span2024/06/20に公開
span2024/06/20
div2024/06/28
div2024/06/28
span2024/06/28
divchatbotOpenAILangChainLLMRAGtech
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。
yamasaKitcheminformatics, machine learning, board gameCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 バッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈るDiscussionyamasaKitcheminformatics, machine learning, board gameバッジを贈るバッジを贈るとは目次背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
divchatbotOpenAILangChainLLMRAGtech
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。
yamasaKitcheminformatics, machine learning, board gameCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 バッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈るDiscussionyamasaKitcheminformatics, machine learning, board gameバッジを贈るバッジを贈るとは目次背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
divchatbotOpenAILangChainLLMRAGtech
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。
yamasaKitcheminformatics, machine learning, board gameCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 バッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈るDiscussionyamasaKitcheminformatics, machine learning, board gameバッジを贈るバッジを贈るとは目次背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
div
div
div
button
svg
path
path
g
a
a
a
divchatbotOpenAILangChainLLMRAGtech
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。
yamasaKitcheminformatics, machine learning, board gameCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 バッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈るDiscussionyamasaKitcheminformatics, machine learning, board gameバッジを贈るバッジを贈るとは目次背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
sectionchatbotOpenAILangChainLLMRAGtech
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。
yamasaKitcheminformatics, machine learning, board gameCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 バッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈るDiscussion
divchatbotOpenAILangChainLLMRAGtech
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。
yamasaKitcheminformatics, machine learning, board gameCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 バッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈る
divchatbotOpenAILangChainLLMRAGtech
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。
yamasaKitcheminformatics, machine learning, board gameCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 バッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈る
divchatbotOpenAILangChainLLMRAGtech
achatbot
div
img
divchatbot
aOpenAI
div
img
divOpenAI
aLangChain
div
img
divLangChain
aLLM
div
img
divLLM
aRAG
div
img
divRAG
atech
div
img
divtech
div
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。

div
 背景
LangChain は OpenAI API を利用し自分たちがやりたいことを実現することに非常に便利なライブラリですがバージョンアップによってクラス名やサブライブラリ名の変更がやや多く少し古い Web 記事を参考にしてもうまくワークしないことがあります。
この記事は 2024/6/20 現在の LangChain (バージョン 0.2.5) で OpenAI API や Azure OpenAI API を動かす例として残しておきます。
同じようなことをしようとして私のように苦戦している方の助けになれば幸いです。

 ソフトウェアのバージョンなど

pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

💡 Poetry で Python のバージョンを指定する時に ^3.12 とすると lancghain-chroma がインストールできなくなるので >=3.12,<3.13 としました。
(langchain-chroma は >=3.12,<3.13 という指定があります)

 方法
OpenAI API を使う場合と AzureOpenAI API を使う場合は基本同じことをするのでまず OpenAI API を使う場合を説明し、記事が長くなってしまったので、後日別記事にて AzureOpenAI API を使う場合はどの部分をアップデートしたらよいのかを説明したいと思います。
6/28 追記) AzureOpenAI API 版の記事も書きましたのでよければぜひどうぞ
https://zenn.dev/cykinso/articles/b055e33734d06b

 .env
以下のように .env を用意します。
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01
もし OPENAI_API_KEY をまだ取得していない場合は以下の方法で取得してください。

 OPENAI_API_KEY
OPENAI_API_KEY は OpenAI の Dashboard で作成できます。
(課金対象なのでご自身の責任のもとご利用ください)

「+ Create new secret key」 を押すとモーダルが開くので後から区別できるような名前をつけて 「Create secret key」 を押します。

表示される API キーを .env にメモしておきます。 「Done」 を押すともう表示できません。


 データ
OpenAI がまだ学習していなさそうなデータの例として弊社 Cykinso のブログ記事の「会社のビジョンを話しているページ」を今回は用いたいと思います。
https://note.com/cykinso/n/n432d5ea70783
💡 プライベートで実装する場合は、好きなマンガなどの詳細をテキストにまとめてデータとするとモチベーションも上がると思います。
ざっと文章をコピーして以下のように整形しました。

note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


 コード
続けてコードを実装します。
今回は RAG として外部の情報を参照しつつ回答する ChatBot を実装してみます。
インターフェースとして streamlit を用います。
先にコード全体を示すと以下のようになります。
(streamlit のコードのベースとして以下の記事を参考にさせていただきました。ありがとうございます)
https://tech-lab.sios.jp/archives/41574

chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

コードの各部分を説明していきます。

 initialize_vector_store
def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store
Vector store とは情報として読み込ませているテキストファイルを適切な長さに分割し(チャンクと呼ばれます) すぐに取り出せるように保存するデータベースのようなものです。
Embeddings と呼ばれるモデルでテキストは数値情報のベクトルに保存されます。その作業を行うために OpenAI API が必要になっているため関数の最初で OpenAIEmbeddings を呼び出しています。
続けて Vector store として保存されているデータがすでに存在していないかを調べています。基本的にデータやモデルがアップデートされない限り Vector store のデータはまったく同じになるため、毎回実行してしまうと時間も API の利用料金も無駄になってしまいます。
そこで

まだ存在していない場合： 新規作成
すでに存在している場合： 保存しているデータベースを読み込む

としています。
なお存在している場合 Chroma クラスの引数 embedding_function に embeddings を指定していますが、これはクエリとしてあたえられるユーザの質問と Vector store に保存されているデータとの間の関連性を調べるために、クエリも Embeddings でベクトルに変換する必要があるからです。

 initialize_retriver
def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()
こちらでは、先ほど作成した(あるいはすでにあるものを読み込んだ) vector_store を retriever に変換しています。こちらの retriever この後、 Vectore store から情報を取り出すのに利用されます。

 initialize_chain
def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain
こちらでは retriever の情報を LLM で利用できるように chain と呼ばれる概念を利用しております。
よくメソッドチェーンという言葉がプログラミング言語では使われています。
例えば Python の Pandas では
import pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)
というようにメソッドの返り値を次のメソッドへとバケツリレーのようにつないで行き処理させることがあります。これをメソッドチェーンといいます。
LangChain でもこのバケツリレーを行い、どのようにユーザの質問に答えるかのルールを決めることができます。
LangChain の場合は昔のバージョンでは関数の返り値をさらに関数の引数にすると言うことを繰り返してバケツリレーを行っていましたが、今のバージョン 0.2.5 では以下のように | を利用しチェーンを示すことが推奨されています。
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
今回の場合

{"context": retriever, "question": RunnablePassthrough()}
prompt
llm

という順番でチェーンがつながっています。
チェーンの先頭がなぜ辞書であるかは、2番目の prompt の説明を聞いてもらえればわかると思います。
    prompt = hub.pull("rlm/rag-prompt")
prompt は LLM にどのような質問や依頼をするのかを決める部分です。今回はプロンプト(変数名を指していない場合カタカナ表記とします)を有志の方がアップロードし他の人が利用できるようにしてくれているサイト LangChain Hub から ⭐ が多いものをお借りしてきました。もちろん自作してもらってもOKです。
https://smith.langchain.com/hub/rlm/rag-prompt
お借りした rag-prompt では以下のようにプロンプトが定義されています。
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
簡単に日本語に訳すと 「 context の情報のみを使って question に答えるように Answer を考えなさい。答えられないならわからないと答えなさい。」 となります。 context の情報のみを用いるよう指定することでハルシネーションを防ぐ効果があります (ただし１００％防ぐとは断言できないです)
こちらの question 部分にユーザのクエリが、 context 部分に retriever を指定して prompt を実行せよとしているのがチェーンの {"context": retriever, "question": RunnablePassthrough()} | prompt の部分です。
最後に完成した prompt を llm に渡しなさいと指定しているのが prompt | llm の部分です。
このメソッドチェーンをまとめた chain という変数は invoke メソッドを持っており、このメソッドに質問を投げるとそれが question に入りチェーンが前から順番に実行されます。
以上のように定義することで RAG として外部の情報を参照しつつ回答する ChatBot を実装できました。

 main
def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")
最後に main 関数です。こちらは LangChain の実装というよりは Streamlit を利用したフロントエンド部分の実装になります。
キーとなる点だけ解説すると

ユーザと ChatBot の会話は messages に保存されています。以下のコードを見るとわかるようにユーザからの質問と ChatBot の返答はどちらも messages に append されています。

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

ユーザからの質問へのレスポンスは先ほど定義した chain の invoke メソッドを用いて作られています。

        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))
という点があげられます。

 テスト
それでは実装したものを動かしてみましょう。
> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501
http://localhost:8501 へブラウザからアクセスし、質問してみます。

現時点での note.txt の先頭の方に書いてあった新しいビジョンをちゃんと答えています。この情報は OpenAI にはないためきちんと RAG が働いていると言えそうです。

また「100年後に発売予定の新商品」という情報として含まないような質問をするとちゃんと「情報を持っていない」と返答してくれます。こちらも期待通りですね。

 💡 まとめ

LangChain v0.2.5 時点での RAG を用いた ChatBot の実装を行いました
Streamlit を用いてブラウザからユーザがアクセスできるようにしました

ぜひ参考にしてみてください。

a
a
div
pyproject.toml
python = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"


divpyproject.toml
spanpyproject.toml
prepython = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

codepython = ">=3.12,<3.13"
python-dotenv = "^1.0.1"
chromadb = "0.5.2"
langchain = "0.2.5"
langchain-cli = "0.0.25"
langchain-openai = "0.1.8"
langchain-community = "0.2.5"
langchain-chroma = "0.1.1"
langchainhub = "0.1.20"
streamlit = "1.35.0"

spanpython
span=
span">=3.12,<3.13"
spanpython-dotenv
span=
span"^1.0.1"
spanchromadb
span=
span"0.5.2"
spanlangchain
span=
span"0.2.5"
spanlangchain-cli
span=
span"0.0.25"
spanlangchain-openai
span=
span"0.1.8"
spanlangchain-community
span=
span"0.2.5"
spanlangchain-chroma
span=
span"0.1.1"
spanlangchainhub
span=
span"0.1.20"
spanstreamlit
span=
span"1.35.0"
a
span
a
divOPENAI_API_KEY=sk-XXXXXXXXXXXXXXXX
OPENAI_API_VERSION=2024-02-01

a
img
img
img
a
span
div
note.txt
細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)


divnote.txt
spannote.txt
pre細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)

code細菌叢からの新たな気付きを通じて、新ビジョンを策定しました！
2023年11月にサイキンソーは10期目に入りました。高齢化社会による社会保障への不安が募る現在、病気を未然に防ぐ0次予防の重要性が高まっています。「細菌叢で人々を健康に」というミッションに向けて、サイキンソーは新たなビジョンを制定しました。
新しいビジョンについて
ービジョン変更で具体的にどの個所が変更したかをまずご紹介します。
こちらがサイキンソーのミッション（MISSION）、ビジョン(VISION)、バリュー(VALUE)になります。
私たちが目指し続ける「細菌叢で人々を健康に」というミッションはそのままに、ビジョンを新しく変更致しました。

＜これまでのビジョン＞
菌叢データから「次世代のライフスタイル」を提供するプラットフォームになる

＜新しいビジョン＞
細菌叢からの新たな気付きを通じて
ヒト、社会、地球環境を健康にするエコシステムを実現する
新しいビジョンでは、サイキンソーが影響を与えていきたい範囲もこれまでよりさらに大きくなったことが分かります。

今回のビジョン変更を通して、サイキンソーがどんな価値発揮を目指していくのか、それに伴い事業面ではどんな挑戦をしていくのかを、代表取締役CEOの沢井さんに聞いてきました！

...(以下略)

a
span
div
chatbot.py
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()


divchatbot.py
spanchatbot.py
prefrom pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

codefrom pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.schema import AIMessage, HumanMessage
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()


def initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store


def initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()


def initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain


def main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")


if __name__ == "__main__":
    main()

spanfrom
spanimport
spanimport
spanas
spanfrom
spanimport
spanfrom
spanimport
spanfrom
span.
spanimport
span,
spanfrom
spanimport
spanfrom
span.
spanimport
spanfrom
span.
spanimport
span,
spanfrom
span.
spanimport
spanfrom
spanimport
span,
spanfrom
spanimport
span(
span)
spandef
spaninitialize_vector_store
span(
span)
span-
span>
span:
span"""VectorStoreの初期化."""
span=
span(
span)
span=
span"./resources/note.db"
spanif
span(
span)
span.
span(
span)
span:
span=
span(
span=
span,
span=
span)
spanelse
span:
span=
span(
span"resources/note.txt"
span)
span=
span.
span(
span)
span=
span(
span=
span1000
span,
span=
span200
span)
span=
span.
span(
span)
span=
span.
span(
span=
span,
span=
span,
span=
span)
spanreturn
spandef
spaninitialize_retriever
span(
span)
span-
span>
span:
span"""Retrieverの初期化."""
span=
span(
span)
spanreturn
span.
span(
span)
spandef
spaninitialize_chain
span(
span)
span-
span>
span:
span"""Langchainの初期化."""
span=
span.
span(
span"rlm/rag-prompt"
span)
span=
span(
span)
span=
span(
span)
span=
span(
span{
span"context"
span:
span,
span"question"
span:
span(
span)
span}
span|
span|
span)
spanreturn
spandef
spanmain
span(
span)
span-
span>
spanNone
span:
span"""ChatGPTを使ったチャットボットのメイン関数."""
span=
span(
span)
span# ページの設定
span.
span(
span=
span"RAG ChatGPT"
span)
span.
span(
span"RAG ChatGPT"
span)
span# チャット履歴の初期化
spanif
span"messages"
spannot
spanin
span.
span:
span.
span.
span=
span[
span]
span# ユーザーの入力を監視
spanif
span:=
span.
span(
span"聞きたいことを入力してね！"
span)
span:
span.
span.
span.
span(
span(
span=
span)
span)
spanwith
span.
span(
span"GPT is typing ..."
span)
span:
span=
span.
span(
span)
span.
span.
span.
span(
span(
span=
span.
span)
span)
span# チャット履歴の表示
span=
span.
span.
span(
span"messages"
span,
span[
span]
span)
spanfor
spanin
span:
spanif
spanisinstance
span(
span,
span)
span:
spanwith
span.
span(
span"assistant"
span)
span:
span.
span(
span.
span)
spanelif
spanisinstance
span(
span,
span)
span:
spanwith
span.
span(
span"user"
span)
span:
span.
span(
span.
span)
spanelse
span:
span.
span(
spanf"System message: {message.content}"
spanf"System message: 
span{message.content}
span{
span.
span}
span"
span)
spanif
span==
span"__main__"
span:
span(
span)
a
divdef initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store

predef initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store

codedef initialize_vector_store() -> Chroma:
    """VectorStoreの初期化."""
    embeddings = OpenAIEmbeddings()

    vector_store_path = "./resources/note.db"
    if Path(vector_store_path).exists():
        vector_store = Chroma(embedding_function=embeddings, persist_directory=vector_store_path)
    else:
        loader = TextLoader("resources/note.txt")
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        vector_store = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory=vector_store_path
        )

    return vector_store

spandef
spaninitialize_vector_store
span(
span)
span-
span>
span:
span"""VectorStoreの初期化."""
span=
span(
span)
span=
span"./resources/note.db"
spanif
span(
span)
span.
span(
span)
span:
span=
span(
span=
span,
span=
span)
spanelse
span:
span=
span(
span"resources/note.txt"
span)
span=
span.
span(
span)
span=
span(
span=
span1000
span,
span=
span200
span)
span=
span.
span(
span)
span=
span.
span(
span=
span,
span=
span,
span=
span)
spanreturn
a
divdef initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()

predef initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()

codedef initialize_retriever() -> VectorStoreRetriever:
    """Retrieverの初期化."""
    vector_store = initialize_vector_store()
    return vector_store.as_retriever()

spandef
spaninitialize_retriever
span(
span)
span-
span>
span:
span"""Retrieverの初期化."""
span=
span(
span)
spanreturn
span.
span(
span)
a
divdef initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain

predef initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain

codedef initialize_chain() -> RunnableSequence:
    """Langchainの初期化."""
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI()
    retriever = initialize_retriever()
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )
    return chain

spandef
spaninitialize_chain
span(
span)
span-
span>
span:
span"""Langchainの初期化."""
span=
span.
span(
span"rlm/rag-prompt"
span)
span=
span(
span)
span=
span(
span)
span=
span(
span{
span"context"
span:
span,
span"question"
span:
span(
span)
span}
span|
span|
span)
spanreturn
divimport pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)

preimport pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)

codeimport pandas as pd

# サンプルデータフレーム
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Edward', 'Frank'],
    'Age': [24, 27, 22, 32, 29, 24],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York'],
    'Score': [85, 90, 88, 92, 95, 70]
}

df = pd.DataFrame(data)

# メソッドチェーンを使ったデータ変換
result = (df
          .query('Age > 25')                     # 年齢が25以上の行をフィルタリング
          .groupby('City')                       # 都市ごとにグループ化
          .agg({'Score': 'mean'})                # スコアの平均を計算
          .rename(columns={'Score': 'Average Score'})  # 列名を変更
          .sort_values(by='Average Score', ascending=False)  # 平均スコアで並べ替え
          .reset_index()                         # インデックスをリセット
         )

print(result)

spanimport
spanas
span# サンプルデータフレーム
span=
span{
span'Name'
span:
span[
span'Alice'
span,
span'Bob'
span,
span'Charlie'
span,
span'David'
span,
span'Edward'
span,
span'Frank'
span]
span,
span'Age'
span:
span[
span24
span,
span27
span,
span22
span,
span32
span,
span29
span,
span24
span]
span,
span'City'
span:
span[
span'New York'
span,
span'Los Angeles'
span,
span'New York'
span,
span'Chicago'
span,
span'Los Angeles'
span,
span'New York'
span]
span,
span'Score'
span:
span[
span85
span,
span90
span,
span88
span,
span92
span,
span95
span,
span70
span]
span}
span=
span.
span(
span)
span# メソッドチェーンを使ったデータ変換
span=
span(
span.
span(
span'Age > 25'
span)
span# 年齢が25以上の行をフィルタリング
span.
span(
span'City'
span)
span# 都市ごとにグループ化
span.
span(
span{
span'Score'
span:
span'mean'
span}
span)
span# スコアの平均を計算
span.
span(
span=
span{
span'Score'
span:
span'Average Score'
span}
span)
span# 列名を変更
span.
span(
span=
span'Average Score'
span,
span=
spanFalse
span)
span# 平均スコアで並べ替え
span.
span(
span)
span# インデックスをリセット
span)
spanprint
span(
span)
div    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )

pre    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )

code    chain = (
        {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
    )

span=
span(
span{
span"context"
span:
span,
span"question"
span:
span(
span)
span}
span|
span|
span)
div    prompt = hub.pull("rlm/rag-prompt")

pre    prompt = hub.pull("rlm/rag-prompt")

code    prompt = hub.pull("rlm/rag-prompt")

span=
span.
span(
span"rlm/rag-prompt"
span)
span
divYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:

preYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:

codeYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:

a
divdef main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")

predef main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")

codedef main() -> None:
    """ChatGPTを使ったチャットボットのメイン関数."""
    chain = initialize_chain()

    # ページの設定
    st.set_page_config(page_title="RAG ChatGPT")
    st.image(img, use_column_width=False)
    st.header("RAG ChatGPT")

    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

    # チャット履歴の表示
    messages = st.session_state.get("messages", [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        else:
            st.write(f"System message: {message.content}")

spandef
spanmain
span(
span)
span-
span>
spanNone
span:
span"""ChatGPTを使ったチャットボットのメイン関数."""
span=
span(
span)
span# ページの設定
span.
span(
span=
span"RAG ChatGPT"
span)
span.
span(
span,
span=
spanFalse
span)
span.
span(
span"RAG ChatGPT"
span)
span# チャット履歴の初期化
spanif
span"messages"
spannot
spanin
span.
span:
span.
span.
span=
span[
span]
span# ユーザーの入力を監視
spanif
span:=
span.
span(
span"聞きたいことを入力してね！"
span)
span:
span.
span.
span.
span(
span(
span=
span)
span)
spanwith
span.
span(
span"GPT is typing ..."
span)
span:
span=
span.
span(
span)
span.
span.
span.
span(
span(
span=
span.
span)
span)
span# チャット履歴の表示
span=
span.
span.
span(
span"messages"
span,
span[
span]
span)
spanfor
spanin
span:
spanif
spanisinstance
span(
span,
span)
span:
spanwith
span.
span(
span"assistant"
span)
span:
span.
span(
span.
span)
spanelif
spanisinstance
span(
span,
span)
span:
spanwith
span.
span(
span"user"
span)
span:
span.
span(
span.
span)
spanelse
span:
span.
span(
spanf"System message: {message.content}"
spanf"System message: 
span{message.content}
span{
span.
span}
span"
span)
div    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

pre    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

code    # チャット履歴の初期化
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ユーザーの入力を監視
    if user_input := st.chat_input("聞きたいことを入力してね！"):
        st.session_state.messages.append(HumanMessage(content=user_input))
        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

span# チャット履歴の初期化
spanif
span"messages"
spannot
spanin
span.
span:
span.
span.
span=
span[
span]
span# ユーザーの入力を監視
spanif
span:=
span.
span(
span"聞きたいことを入力してね！"
span)
span:
span.
span.
span.
span(
span(
span=
span)
span)
spanwith
span.
span(
span"GPT is typing ..."
span)
span:
span=
span.
span(
span)
span.
span.
span.
span(
span(
span=
span.
span)
span)
div        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

pre        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

code        with st.spinner("GPT is typing ..."):
            response = chain.invoke(user_input)
        st.session_state.messages.append(AIMessage(content=response.content))

spanwith
span.
span(
span"GPT is typing ..."
span)
span:
span=
span.
span(
span)
span.
span.
span.
span(
span(
span=
span.
span)
span)
a
div> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501

pre> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501

code> streamlit run chatbot.py

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://10.200.0.4:8501
  External URL: http://13.73.233.61:8501

span>
spanset
spanin
img
img
a
div
div
button
svg
path
path
g
div
button
a
a
a
asideyamasaKitcheminformatics, machine learning, board game
divyamasaKitcheminformatics, machine learning, board game
a
img
divyamasaKit
ayamasaKit
divcheminformatics, machine learning, board game
pcheminformatics, machine learning, board game
div
span
a
a
a
divCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 
divCykinso's Tech BlogPublication「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 
a
img
divCykinso's Tech BlogPublication
aCykinso's Tech Blog
aPublication
div「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 
p「細菌叢の力で人々を健康に」をミッションに掲げるバイオテックスタートアップ「サイキンソー」の技術ブログ。 
div
span
a
a
asideバッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈る
divバッジを贈って著者を応援しようバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。バッジを贈る
divバッジを贈って著者を応援しよう
pバッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。
divバッジを贈る
buttonバッジを贈る
svg
divDiscussion
sectionDiscussion
divDiscussion
h3Discussion
img
asideyamasaKitcheminformatics, machine learning, board gameバッジを贈るバッジを贈るとは目次背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
divyamasaKitcheminformatics, machine learning, board gameバッジを贈るバッジを贈るとは目次背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
divyamasaKitcheminformatics, machine learning, board gameバッジを贈るバッジを贈るとは
divyamasaKit
img
divyamasaKit
ayamasaKit
div
a
a
a
pcheminformatics, machine learning, board game
buttonバッジを贈る
divバッジを贈るとは
aバッジを贈るとは
svg
div目次背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
div目次背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
div目次
div背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
ol背景ソフトウェアのバージョンなど方法.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmainテスト💡 まとめ
ol.envデータコードinitialize_vector_storeinitialize_retriverinitialize_chainmain
footerZennエンジニアのための情報共有コミュニティAboutZennについて運営会社お知らせ・リリースGuides使い方Publication / ProNewよくある質問LinksX(Twitter)GitHubメディアキットLegal利用規約プライバシーポリシー特商法表記
divZennエンジニアのための情報共有コミュニティAboutZennについて運営会社お知らせ・リリースGuides使い方Publication / ProNewよくある質問LinksX(Twitter)GitHubメディアキットLegal利用規約プライバシーポリシー特商法表記
divZennエンジニアのための情報共有コミュニティAboutZennについて運営会社お知らせ・リリースGuides使い方Publication / ProNewよくある質問LinksX(Twitter)GitHubメディアキットLegal利用規約プライバシーポリシー特商法表記
divZennエンジニアのための情報共有コミュニティ
path
path
pエンジニアのための情報共有コミュニティ
divAboutZennについて運営会社お知らせ・リリースGuides使い方Publication / ProNewよくある質問LinksX(Twitter)GitHubメディアキットLegal利用規約プライバシーポリシー特商法表記
navAboutZennについて運営会社お知らせ・リリース
h4About
navGuides使い方Publication / ProNewよくある質問
h4Guides
spanNew
navLinksX(Twitter)GitHubメディアキット
h4Links
navLegal利用規約プライバシーポリシー特商法表記
h4Legal
div
div
div
